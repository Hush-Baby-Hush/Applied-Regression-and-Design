---
title: "STAT 425 Assignment 2"
output:
  pdf_document: default
  html_document:
    df_print: paged
fontsize: 12pt
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
Due Monday, February 22, 11:59pm. Submit through Moodle.

## Name: (insert your name here)
### Netid: (insert)

Submit your computational work both as an R markdown (*.Rmd) document and as a pdf, along with any files needed to run the code. Embed your answers to each problem in the document below after the question statement. If you have hand-written work, please scan or take pictures of it and include in a pdf file, ideally combined with your pdf output file from R Markdown.

## Problem 1

In this problem we have data of the form $(x_1, y_1), (x_2, y_2), \dots, (x_n, y_n)$. We construct a new variable $z_i$ from the $x_i$ values by subtracting their sample mean, so $z_i = x_i - \bar{x}$ for $i=1,2,\ldots, n$, where $\bar{x}_i = n^{-1}\sum_{i=1}^n x_i$. We consider two models:
\begin{eqnarray*}
\mbox{Model 1:}&& y_i = \beta_0 + \beta_1 x_i + e_i, \quad i=1,2,\ldots,n   \\
\mbox{Model 2:}&& y_i = \alpha_0 + \alpha_1 z_i + e_i, 
\quad i=1,2,\ldots,n 
\end{eqnarray*}
Here the $x_i$ values are considered a fixed set of numbers, and the errors $e_i$ are considered to be uncorrelated random variables with $E(e_i)=0$ and $\mbox{var}(e_i) = \sigma^2$. 


**a)** By subtracting one model from the other and averaging, show that $\alpha_0 = \beta_0 + \beta_1 \bar{x}$.

**Answer:** Since $z_i = x_i-\bar{x}$, by subtracting one model from the other yields
\begin{align*}
  \beta_0 +\beta_1 x_i = \alpha_0 +\alpha_1 (x_i-\bar{x}), \quad i = 1, 2, \ldots, n.
\end{align*}
Summing the above equation for $i = 1, \ldots, n$ and taking average yields
\begin{align*}
  \beta_0 + \beta_1 \bar{x} = \alpha_0,
\end{align*}
which follows from the fact that
\begin{align*}
 \frac{1}{n} \sum_{i=1}^n (x_i-\bar{x})= \bar{x}-\bar{x}=0
 \quad \text{ and }
 \quad \frac{1}{n} \sum_{i=1}^n x_i = \bar{x}.
\end{align*}
It finishes the proof.

(here or indicate where it is in the attached pdf file.)


**b)** Substituting $\alpha_0 = \beta_0 + \beta_1 \bar{x}$ into Model 2, show that $(\alpha_1 - \beta_1)(x_i - \bar{x})=0$, for $i=1,2,\ldots,n$. Assume $\sum_{i=1}^n (x_i - \bar{x})^2 >0$, i.e., the $x_i$ values are not all the same. Show why this implies $\alpha_1 = \beta_1$.

**Answer:** Substituting $\alpha_0 = \beta_0 + \beta_1 \bar{x}$ into Model 2 yields
\begin{align*}
  y_i &= \beta_0 + \beta_1 \bar{x} + \alpha_1 z_i + e_i\\
  &=\beta_0 + \beta_1 \bar{x} + \alpha_1 (x_i-\bar{x}) + e_i, 
  \quad i = 1, \ldots, n. \tag{$z_i = x_i-\bar{x}$} 
\end{align*}
Subtituting $y_i$ in the right hand side of the above equation with $y_i = \beta_0 + \beta_1 x_i+e_i$ as in Model 1 shows that
\begin{align*}
  \beta_0 + \beta_1 x_i + e_i
  &=\beta_0 + \beta_1 \bar{x} + \alpha_1 (x_i-\bar{x}) + e_i,
   \quad i = 1, \ldots, n.\tag{$z_i = x_i-\bar{x}$} 
\end{align*}
Cancelling terms results in
\begin{align*}
  \beta_1 x_i 
  &= \beta_1 \bar{x} + \alpha_1 (x_i-\bar{x}) 
  \Rightarrow (\alpha_1-\beta_1) (x_i-\bar{x})=0, \quad i = 1, \ldots, n.
  \end{align*}
  It further implies
  \begin{align*}
    (\alpha_1-\beta_1)^2 (x_i-\bar{x})^2 = 0, \quad i = 1, \ldots, n.
  \end{align*}
Summing the above equation over $i = 1, \ldots,n$  yields
\begin{align*}
  (\alpha_1-\beta_1)^2 \sum_{i=1}^n (x_i-\bar{x})^2 = 0.
\end{align*}
Because by assumption $\displaystyle \sum_{i=1}^n (x_i-\bar{x})^2  > 0$, the above equation implies  $\alpha_1=\beta_1$.


Now we use matrix notation and rewrite Model 2 as 
$\mathbf{y} = \mathbf{Z}\mathbf{\alpha}+\mathbf{e}$ with:

$$
\mathbf{y}=\begin{pmatrix}y_1 \\ y_2, \\ \vdots\\ y_n\end{pmatrix}\qquad
\mathbf{Z}=\begin{pmatrix}
1 & z_1\\ 
1 & z_2 \\ 
\vdots & \vdots\\
1 & z_n\end{pmatrix}\qquad
\mathbf{\alpha} = \begin{pmatrix}\alpha_1 \\ \alpha_2\end{pmatrix}\qquad
\mathbf{e} = \begin{pmatrix}e_1 \\ e_2, \\ \vdots\\ e_n\end{pmatrix}
$$

**c)** Show 
$$
\mathbf{Z}^T \mathbf{Z}=
\begin{pmatrix}n & 0 \\ 0 & S_{xx}\end{pmatrix}
$$
where $S_{xx} = \sum_{i=1}^n (x_i - \bar{x})^2$.



**Answer:** We can write
\begin{align*}
  \mathbf{Z}^{T}\mathbf{Z}=
  \begin{pmatrix}
    1 & 1 & \cdots & 1 \\
    z_1 & z_2 & \cdots & z_n
  \end{pmatrix}
  \begin{pmatrix}
    1 & z_1 \\
    1 & z_2 \\
    \vdots & \vdots \\
    1 & z_n
  \end{pmatrix}
  = \begin{pmatrix}
    n & \displaystyle \sum_{i=1}^n z_i \\
    \displaystyle \sum_{i=1}^n z_i & \displaystyle \sum_{i=1}^n z_i^2
  \end{pmatrix}.
\end{align*}
Because
\begin{align*}
  \sum_{i=1}^n z_i &= \sum_{i=1}^n (x_i-\bar{x}) = \sum_{i=1}^n x_i - n \bar{x}= \sum_{i=1}^n x_i- \sum_{i=1}^n x_i=0 \\
  \sum_{i=1}^n z_i^2 &=  \sum_{i=1}^n (x_i-\bar{x}) ^2 = S_{xx},
\end{align*}
we can obtain
\begin{align*}
  \mathbf{Z}^{T}\mathbf{Z}
  = \begin{pmatrix}
    n & 0 \\
   0 &  S_{xx}
  \end{pmatrix}.
\end{align*}


**d)** The LS estimate of the coefficient vector $\mathbf{\alpha}$ solves the matrix equation:
$$
\mathbf{Z}^T\mathbf{Z}\,\hat{\mathbf{\alpha}}= \mathbf{Z}^T \mathbf{y}
$$
Solve the equation algebraically to get simplified expressions for $\hat{\alpha}_1$ and $\hat{\alpha}_2$ expressed in terms of the original $x_i$, $y_i$ and $n$ values.


**Answer:** From c), for the equation $\mathbf{Z}^{T}\mathbf{Z}\hat{\boldsymbol{\alpha}}=\mathbf{Z}^{T}\mathbf{y}$,  we can  expand it to 
\begin{align*}
  \begin{pmatrix}
    n & 0 \\
   0 &  S_{xx}
  \end{pmatrix} \begin{pmatrix}
    \hat{\alpha}_1 \\ \hat{\alpha}_2
  \end{pmatrix}
  &= \begin{pmatrix}
    1 & 1 & \cdots & 1 \\
    z_1 & z_2 & \cdots & z_n
  \end{pmatrix} \begin{pmatrix}
    y_1 \\ \vdots \\ y_n
  \end{pmatrix} \\
  &= \begin{pmatrix}
  \displaystyle   \sum_{i=1}^n y_i \\
   \displaystyle   \sum_{i=1}^n (x_i-\bar{x}) y_i
  \end{pmatrix} \tag{$z_i = x_i-\bar{x}$}.
\end{align*}
The above equation further implies 
\begin{align*}
  \hat{\alpha}_1 &= \frac{1}{n} \sum_{i=1}^n y_i, \\
  \hat{\alpha}_2 &= \frac{\displaystyle \sum_{i=1}^n (x_i-\bar{x}) y_i}{S_{xx}}
  =  \frac{\displaystyle\sum_{i=1}^n (x_i-\bar{x}) y_i}{\displaystyle\sum_{i=1}^n (x_i-\bar{x})^2}.
\end{align*}
The above two identities are the simplified expressions for $\hat{\alpha}_1$ and $\hat{\alpha}_2$.


**e)** Derive a simplified expressions for the entries of $\mbox{cov}(\hat{\mathbf{\alpha}})$, the $2\times 2$ covariance matrix of $\hat{\mathbf{\alpha}}=\begin{pmatrix}\hat{\alpha}_1\\ \hat{\alpha}_2\end{pmatrix}$. (Note that this still depends on the unknown $\sigma^2$.)

**Answer:** 
It suffices to compute $\mathrm{var}(\hat{\alpha}_1)$, $\mathrm{var}(\hat{\alpha}_2)$, and $\mathrm{cov}(\hat{\alpha}_1, \hat{\alpha}_2)$. Note that  by the assumption that $e_i$ and $e_j$ are uncorrelated for $i \neq j$,
\begin{align*}
  \mathrm{var}(\hat{\alpha}_1) 
  &=  \mathrm{var}(\bar{y})=\frac{\sigma^2}{n}, \\
   \mathrm{var}(\hat{\alpha}_2) 
   &=  \frac{\displaystyle\sum_{i=1}^n (x_i-\bar{x})^2 \cdot \mathrm{var}(y_i)}{\left(\displaystyle\sum_{i=1}^n (x_i-\bar{x})^2\right)^2}
   = \sigma^2\frac{\displaystyle\sum_{i=1}^n (x_i-\bar{x})^2 }{\left(\displaystyle\sum_{i=1}^n (x_i-\bar{x})^2\right)^2}
  = \frac{\sigma^2}{\displaystyle\sum_{i=1}^n (x_i-\bar{x})^2}.
\end{align*}
By linearity property of covariance,
\begin{align*}
  \mathrm{cov}(\hat{\alpha}_1 , \hat{\alpha}_2) 
  &= 
  \mathrm{cov}\left(\frac{1}{n} \sum_{i=1}^n y_i, \frac{\displaystyle\sum_{i=1}^n (x_i-\bar{x}) y_i}{\displaystyle\sum_{i=1}^n (x_i-\bar{x})^2}\right) \\
  &=  \frac{1}{\displaystyle n\sum_{i=1}^n (x_i-\bar{x})^2} \cdot  \mathrm{cov}\left( \sum_{i=1}^n y_i,\sum_{i=1}^n (x_i-\bar{x}) y_i\right) .
\end{align*}
Now, we further simplifies the last covariance term as below.
\begin{align*}
  \mathrm{cov}\left( \sum_{i=1}^n y_i,\sum_{i=1}^n (x_i-\bar{x}) y_i\right) 
  &= \sum_{i=1}^n (x_i-\bar{x}) \mathrm{var}(y_i) + \sum_{i\neq j} (x_j-\bar{x}) \cdot \mathrm{cov}(y_i, y_j) \\
  &=\sigma^2 \sum_{i=1}^n (x_i-\bar{x}) + \sum_{i\neq j} (x_j-\bar{x}) \cdot 0 \tag{$\mathrm{cov}(y_i, y_j)=\mathrm{cov}(e_i, e_j)=0$} \\
  &= 0. \tag{$\displaystyle \sum_{i=1}^n (x_i-\bar{x}) =0$}
\end{align*}
Thus, it shows that $\mathrm{cov}(\hat{\alpha}_1 , \hat{\alpha}_2)=0$. As a result,
\begin{align*}
  \mathrm{cov}(\hat{\boldsymbol{\alpha}}) =
  \begin{pmatrix}
   \sigma^2/n & 0 \\
    0 & \displaystyle \sigma^2 \bigg/ \sum_{i=1}^n (x_i-\bar{x})^2
  \end{pmatrix}.
\end{align*}

\textbf{Alternative solution:} 

From general results about LS regression,
\begin{align*}
  \mathrm{cov}(\hat{\boldsymbol{\alpha}}) = \sigma^2 (\mathbf{Z}^T \mathbf{Z})^{-1}.
\end{align*}
Using Part c we obtain
\begin{align*}
  \sigma^2 (\mathbf{Z}^T \mathbf{Z})^{-1} 
= \sigma^2 
  \begin{pmatrix}
    n^{-1} & 0 \\
                     0 & S_{xx}^{-1}
  \end{pmatrix}
\end{align*}

## Problem 2

This problem considers the \texttt{prostate} data in the library \texttt{faraway}. See \texttt{help(prostate)} for more information about the data set.

**a)** Fit a linear model with \texttt{lpsa} as the response and all the other variables as predictor variables. Display the model summary and identify all variables whose coefficient t values are statistically significant at the $\alpha=0.05$ level. In each case, state what the null hypothesis is for the t test.

**Answer:**

```{r}
library(faraway)
fit_2a = lm(lpsa ~ ., data = prostate)
summary(fit_2a)
```

The lcavol, lweight, and svi predictors are statistically significant at the $\alpha=0.05$ significance level. The null hypothesis in each case is that the coefficient of a single predictor is 0 and that there is no linear association between this predictor and the lpsa response variable after adjusting for all other predictors.


**b)** Fit a reduced model with \texttt{lpsa} as the response but removing \texttt{lcp}, \texttt{gleason}, and \texttt{pgg45} as predictors in the model. Display the model summary, and provide both 90\% and 95\% confidence intervals for the coefficient of \texttt{lbph}. Do either or both of them include 0?

**Answer:**

```{r}
fit_2b = lm(lpsa ~ lcavol + lweight + age + lbph + svi, data = prostate)
summary(fit_2b)
confint(fit_2b, 'lbph', level = .95)
confint(fit_2b, 'lbph', level = .90)
```

Only the 95% confidence interval includes 0. This makes sense because the p-value of the t-test for the lbph predictor outputted in the summary is low enough to reject at the 90% confidence level, but not at the 95% confidence level.

**c)** Perform an F test comparing the model from Part b) as the null model, and the model from Part a) as the alternative model. Based on your calculations, does the test reject or accept the null hypothesis at the level $\alpha=0.05$? 

**Answer:**

```{r}
anova(fit_2b, fit_2a)
```

With a p-value of 0.4421, this test fails to reject the null hypothesis at the $\alpha=0.05$ level. We accept the reduced model over the full model.

**d)** Here are the data for observation \#32:
\begin{verbatim}
> prostate[32,]
      lcavol lweight age     lbph svi      lcp gleason pgg45    lpsa
32 0.1823216  6.1076  65 1.704748   0 -1.38629       6     0 2.00821
\end{verbatim}

Pretending you did not know the value for \texttt{lpsa} for this observation, use your model from Part b) to compute a 95\% prediction interval for observation 32 based on the predictors in the model. Does the interval include the value observed in the data?

**Answer:**

```{r}
predict(fit_2b, newdata = prostate[32,], interval = "prediction", level = .95)
prostate[32,]$lpsa
```

This interval does include the observed value.

**e)** It is often useful to make a scatter plot of the residuals, 
$\mathbf{r} = \mathbf{y} - \hat{\mathbf{y}}$, on the vertical axis versus the fitted values,
$\hat{\mathbf{y}}$ on the horizontal axis. Make such a scatter plot for the model in Part b), and add a horizontal line at a vertical height 0. Hint: \texttt{abline(h=0)}. Is there any trend or pattern, or does the point cloud appear random without any systematic trend or curvature? Explain briefly.

**Answer:**

```{r}
graph = plot(fit_2b$fitted.values, fit_2b$residuals,
             xlab = "y hat",
             ylab = "Residuals") +
  abline(h = 0)
```

There seems to be no clear trend or pattern between residuals and fitted values. The residuals seem to be fairly clustered around the residuals = 0 line regardless of the y hat value and there appears to be no extreme outliers.






## Problem 2

This problem considers the \texttt{prostate} data in the library \texttt{faraway}. See \texttt{help(prostate)} for more information about the data set.

**a)** Fit a linear model with \texttt{lpsa} as the response and all the other variables as predictor variables. Display the model summary and identify all variables whose coefficient t values are statistically significant at the $\alpha=0.05$ level. In each case, state what the null hypothesis is for the t test.

**Answer:**

```{r 2a}
library(faraway)
fit_2a = lm(lpsa ~ ., data = prostate)
summary(fit_2a)
```

The lcavol, lweight, and svi predictors are statistically significant at the $\alpha=0.05$ significance level. The null hypothesis in each case is that the coefficient of a single predictor is 0 and that there is no linear association between this predictor and the lpsa response variable after adjusting for all other predictors.


**b)** Fit a reduced model with \texttt{lpsa} as the response but removing \texttt{lcp}, \texttt{gleason}, and \texttt{pgg45} as predictors in the model. Display the model summary, and provide both 90\% and 95\% confidence intervals for the coefficient of \texttt{lbph}. Do either or both of them include 0?

**Answer:**

```{r 2b}
fit_2b = lm(lpsa ~ lcavol + lweight + age + lbph + svi, data = prostate)
summary(fit_2b)
confint(fit_2b, 'lbph', level = .95)
confint(fit_2b, 'lbph', level = .90)
```

Only the 95% confidence interval includes 0. This makes sense because the p-value of the t-test for the lbph predictor outputted in the summary is low enough to reject at the 90% confidence level, but not at the 95% confidence level.

**c)** Perform an F test comparing the model from Part b) as the null model, and the model from Part a) as the alternative model. Based on your calculations, does the test reject or accept the null hypothesis at the level $\alpha=0.05$? 

**Answer:**

```{r 2c}
anova(fit_2b, fit_2a)
```

With a p-value of 0.4421, this test fails to reject the null hypothesis at the $\alpha=0.05$ level. We accept the reduced model over the full model.

**d)** Here are the data for observation \#32:
\begin{verbatim}
> prostate[32,]
      lcavol lweight age     lbph svi      lcp gleason pgg45    lpsa
32 0.1823216  6.1076  65 1.704748   0 -1.38629       6     0 2.00821
\end{verbatim}

Pretending you did not know the value for \texttt{lpsa} for this observation, use your model from Part b) to compute a 95\% prediction interval for observation 32 based on the predictors in the model. Does the interval include the value observed in the data?

**Answer:**

```{r 2d}
predict(fit_2b, newdata = prostate[32,], interval = "prediction", level = .95)
prostate[32,]$lpsa
```

This interval does include the observed value.

**e)** It is often useful to make a scatter plot of the residuals, 
$\mathbf{r} = \mathbf{y} - \hat{\mathbf{y}}$, on the vertical axis versus the fitted values,
$\hat{\mathbf{y}}$ on the horizontal axis. Make such a scatter plot for the model in Part b), and add a horizontal line at a vertical height 0. Hint: \texttt{abline(h=0)}. Is there any trend or pattern, or does the point cloud appear random without any systematic trend or curvature? Explain briefly.

**Answer:**

```{r 2e}
graph = plot(fit_2b$fitted.values, fit_2b$residuals,
             xlab = "y hat",
             ylab = "Residuals") +
  abline(h = 0)
```

There seems to be no clear trend or pattern between residuals and fitted values. The residuals seem to be fairly clustered around the residuals = 0 line regardless of the y hat value and there appears to be no extreme outliers.





## Problem 3:

This problem refers to the \texttt{punting} data in the \texttt{faraway} library. The average distance punted and hang times of 10 punts of a football were measured for 13 volunteers. The left and right leg strength and flexibility were also recorded for each volunteer. 

**a)** Fit a regression model with
  \texttt{Distance} as the response, and \texttt{RStr},
  \texttt{LStr}, \texttt{RFlex} and \texttt{LFlex} as predictors (left and right leg strength, and left and right leg flexibility).
  Present a summary of the fitted model. Which if any predictors are significant at the 5\% level?

**Answer:**
```{r}
library("faraway")
model = lm(Distance~RFlex + LFlex + LStr + RStr, data = punting)
summary(model)
```

As we can see from p value column in the summary of the model, none of the variables are significant at this level since they are all greater than 0.05. 


**b)** Use an F-test to determine whether collectively these four predictors have any 
  relationship with the response, i.e., test the (null) hypothesis that
    $\beta_\text{\texttt{RStr}} = \beta_\text{\texttt{LStr}}=
      \beta_\text{\texttt{RFlex}}=\beta_\text{\texttt{LFlex}}=0$. (Here we are referring to the coefficient for predictor $X_j$ in the model as $\beta_{X_j}$.) What do you conclude?

**Answer:**
```{r}
modelreduce = lm(Distance~1 , data  = punting)
anova(modelreduce, model)
```

The p value is smaller than 0.05, which means we reject the null hypothesis that all the coefficients are 0. 

**c)** Now we wish to test whether $\beta_\text{\texttt{RStr}} = \beta_\text{\texttt{LStr}}$ but not necessarily 0. 
  Under the hypothesis that these two coefficients are equal write out the regression model formula and show that 
  it is equivalent to replacing \texttt{RStr} and \texttt{LStr} in the model by the single variable 
  \texttt{Str} = \texttt{RStr} + \texttt{LStr}.


**Answer:**
If these two coefficients are equal ($\beta_{RStr} = \beta_{LStr} = \beta_{Str}$), the regression model could be written as:
\begin{align*}
y &= \beta_{RStr}x_{RStr} + \beta_{LStr}x_{LStr} + \beta_{RFlex}x_{RFlex} + \beta_{LFlex}x_{LFlex} + e\\
&= \beta_{Str}x_{RStr} + \beta_{Str}x_{LStr} + \beta_{RFlex}x_{RFlex} + \beta_{LFlex}x_{LFlex} + e\\ 
&= \beta_{Str}(x_{RStr} + x_{LStr}) + \beta_{RFlex}x_{RFlex} + \beta_{LFlex}x_{LFlex} + e\\ 
&= \beta_{Str}x_{Str} + \beta_{RFlex}x_{RFlex} + \beta_{LFlex}x_{LFlex} + e\\
\end{align*}

We can see it is equivalent to substitute these two variables with a single one by defining the new one "Str" as sum of "RStr" and "LStr". 

**d)** Use an $F$-test to test whether $\beta_\text{\texttt{RStr}} = \beta_\text{\texttt{LStr}}$. 
  Note that the reduced model implied by this hypothesis entails replacing \texttt{RStr} and \texttt{LStr} in the
   \texttt{lm} model formula by \texttt{I(RStr+LStr)} (using the syntax of  \textsf{R}). 

**Answer:**
```{r}
modelred = lm(Distance ~ I(RStr + LStr) + RFlex + LFlex, data = punting)
anova(modelred, model)
```

From the test result, p value is much larger than 0.05. The result shows the the null hypothesis is acceptable. Therefore, we can accept these two coefficients are equal. 


**e)** Make a plot of residuals versus fitted values for the reduced model considered in Part d). Does the plot show any trend or pattern, or does it appear to be random noise? Explain briefly.

**Answer:**
```{r}
plot(modelred, which = 1)
```



From the plot we can see there is an obvious trend. But the points seem to be evenly spread around the trend so the noise appears to be random. 



